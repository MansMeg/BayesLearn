% Bayesian learning: Assignment 4
% Måns Magnusson;Josef Wilzén
% December 2012
```{r startchunk,echo=FALSE,message=FALSE}
# Options
options(width=65)

# Libraries
library(ggplot2)
library(pander)
library(mvtnorm)
library(geoR)
library(msm)


```

```{r "Read data",echo=FALSE}
# Read data
setwd("~/Desktop/BayesLearn/")


```

## Question a)

The Metropolis algorithm (based on a MV normal proposal distribution) were implemented in R in the following way (as a block version):


```{r "M-H algo"}
# The Metropolis-Hasting algorithm with the following inputs:
# logPostFunc: Posterior function (prop to density)
# start.theta: vector of starting values for parameters
# no.sim: Number of draws

MH.algo<-function(logPostFunc,start.theta,no.sim,...){
  library(mvtnorm)
  # Create matrix to store results
  par.draws <- matrix(0,ncol=length(start.theta),nrow=no.sim)
  propose.theta<-numeric(length(start.theta))
  
  # Set starting values
  par.draws[1,] <- start.theta
  sig<-diag(ncol(par.draws))
  no.accept<-0
  
  for (j in 2:no.sim){
    # Propose theta
    propose.theta <- rmvnorm(1,mean=par.draws[j-1,],sigma=sig)
  
    # Calculate r
    suppressWarnings(
      log.r <- logPostFunc(propose.theta, ...) - 
        logPostFunc(par.draws[j-1,], ...)
    )
    if(is.nan(log.r)){log.r<--Inf}

    # Accept/Reject
    if (runif(1) < min(exp(log.r),1) ){
      par.draws[j,]<-propose.theta
      no.accept<-no.accept+1
    }else{
      par.draws[j,]<-par.draws[j-1,]
    }
    
    # Changing proposing sigma to reach an acceptance prob of 0.3
    if(j %% 20 == 0){
      if(no.accept < 2){sig<-0.9*sig}
      if(no.accept > 11){sig<-1.25*sig}
      no.accept<-0
    }  
  }
  return(par.draws)
}
```

To be able to get an acceptance rate of 0.3 that has been shown to be a good acceptance proportion $\sigma$ in the proposal distribution is increased (by 25 %) or decreased (by 10 %) if the acceptance rate is too low (< ```r 1/20```) or to high (> ```r 11/20```). 

Initially this function were computed so that if the value was erroneous (i.e. outside [0,1]) the function returned an r of 0. So in this case the erroneous proposed value is rejected by the MH algorithm. 

Why do you need to use the logit transform if we already (after a while) get an acceptance rate of 0.3?


## Question b)

Initially the code given at the home page were used and read into R.

```{r}
source("http://www.ida.liu.se/~732A46/2012/Labs/dbetaLogit.R")
```

Based on the logit function and the Metropolis algorithm given in exercise a) values of of the posterior were simulated at the logit (real) line an the values of theta were then transfomed back to the unit interval as follows:


```{r}
# Metropolis simulation
logit.mod<-MH.algo(logPostFunc=dbetaLogit,
              start.theta=0.5,
              no.sim=1000,
              shape1 = 10 + 3,
              shape2 = 3 + 3,
              log=T)

# Transform back
logit.mod <- 1/(1+exp(-logit.mod))

```

The plot of the sampled values of $\theta$ can be seen in the following plot.

```{r,echo=FALSE}
# Plot the values
plot(logit.mod,type="l")
```

## Question c)

The data from the previous laboration were loaded into R. 

```{r,echo=FALSE}
# Load former lab data and code
setwd("~/Desktop/BayesLearn/Assign3/")

Probit <- 0
chooseCov <- c(1:16)
tau <- 10
source("MainOptimizeSpamMansJosef.R")

setwd("~/Desktop/BayesLearn/")
```

The logitic model posterior function were altered slightly to be able to use it from the ```ML-algo``` function in exercise a) above. The new likelihood can be seen below:

```{r LogPostLogistic}
LogPostLogistic <- function(theta,y,X,mu,Sigma){
  theta<-as.numeric(theta)
  nPara <- length(theta)
  linPred <- X%*%theta
                                      
  logLik <- sum( linPred*y -log(1 + exp(linPred)))
  # Likelihood is not finite, stear the optimizer away from here!
  if (abs(logLik) == Inf) logLik = -20000
  logPrior <- dmvnorm(theta, matrix(0,nPara,1), Sigma, log=TRUE)
  return(logLik + logPrior)
}
```

Using this likelihood and the function given i a) the (block version) Metropolis function were used and 100000 simulations were done.


```{r BetaSim}
beta.vec<-MH.algo(logPostFunc=LogPostLogistic,
        start.theta=numeric(16),
        no.sim=10000,
        y=y,X=X,mu=mu,Sigma=Sigma)
```

These simulations had a hard time to sample from the distribution of the $\beta$ vector since the proposed values were evaluated as a whole block, with the same prosed distribution. The peoblem with this approach can be seen in the following two plots.


```{r,echo=FALSE}
plot(beta.vec[,8],type="l")
plot(beta.vec[,10],type="l")
```

To conter this problem a new MH algorith were created were the posterior is not evaluated for the whole block of parameter values but cycling through each parameter as is done in the gibbs sampler.

