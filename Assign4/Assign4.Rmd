% Bayesian learning: Assignment 4
% Måns Magnusson;Josef Wilzén
% December 2012
```{r startchunk,echo=FALSE,message=FALSE}
# Options
options(width=65)

# Libraries
library(ggplot2)
library(pander)
library(mvtnorm)
library(geoR)
library(msm)


```

```{r "Read data",echo=FALSE}
# Read data
setwd("~/Desktop/BayesLearn/Assign4")


```

# Question a)

The Metropolis algorithm (based on a MV normal proposal distribution) were implemented in R in the following way:


```{r "M-H algo"}
# The Metropolis-Hasting algorithm with the following inputs:
# logPostFunc: Posterior function (prop to density)
# start.theta: vector of starting values for parameters
# no.sim: Number of draws

MH.algo<-function(logPostFunc,start.theta,no.sim,...){
  library(mvtnorm)
  # Create matrix to store results
  par.draws <- matrix(0,ncol=length(start.theta),nrow=no.sim)
  propose.theta<-numeric(length(start.theta))
  
  # Set starting values
  par.draws[1,] <- start.theta
  sig<-diag(ncol(par.draws))
  no.accept<-0
  
  for (j in 2:no.sim){
    # Propose theta
    propose.theta <- rmvnorm(1,mean=par.draws[j-1,],sigma=sig)
  
    # Calculate r
    suppressWarnings(
      log.r <- logPostFunc(propose.theta, ...) - 
        logPostFunc(par.draws[j-1,], ...)
    )
    if(is.nan(log.r)){log.r<--Inf}

    # Accept/Reject
    if (runif(1) < min(exp(log.r),1) ){
      par.draws[j,]<-propose.theta
      no.accept<-no.accept+1
    }else{
      par.draws[j,]<-par.draws[j-1,]
    }
    
    # Changing proposing sigma to reach an acceptance prob of 0.3
    if(j %% 20 == 0){
      if(no.accept < 2){sig<-0.9*sig}
      if(no.accept > 11){sig<-1.25*sig}
      no.accept<-0
    }  
  }
  return(par.draws)
}
```

To be able to get an acceptance rate of 0.3 that has been shown to be a good acceptance proportion $sigma$ in the proposal distribution is increased (by 25 %) or decreased (by 10 %) if the acceptance rate is too low (< ```r 1/20```) or to high (> ```r 11/20```). 

Initially this function were computed so that if the value was erroneous (i.e. outside [0,1]) the function returned an r of 0. So in this case the erroneous proposed value is rejected by the MH algorithm. 

Why do you need to use the logit transform if we already (after a while) get an acceptance rate of 0.3?


# Question b)

Initially the code given at the home page were used and read into R.

```{r}
source("http://www.ida.liu.se/~732A46/2012/Labs/dbetaLogit.R")
```

Based on the logit function and the Metropolis algorithm given in exercise a) values of of the posterior were simulated at the logit (real) line an the values of theta were then transfomed back to the unit interval.


```{r}
# Metropolis simulation
logit.mod<-MH.algo(logPostFunc=dbetaLogit,
              start.theta=0.5,
              no.sim=1000,
              shape1 = 10 + 3,
              shape2 = 3 + 3,
              log=T)

# Transform back
logit.mod <- 1/(1+exp(-logit.mod))

# Plot the values
plot(logit.mod,type="l")

```


